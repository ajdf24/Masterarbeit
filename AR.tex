\section{Augmented Reality}
In der heutigen Zeit ist fast jedem der Begriff Augmented Reality geläufig. Jedoch gibt es im wissenschaftlichen Umfeld keine einheitliche Definition. Georg Klein definiert die \ac{AR} als "`Anreicherung der realen Welt um computergenerierte Zusatzobjekte"'. \cite{KleinG}

Viele Abhandlungen zu diesem Thema beziehen sich auf das "`Reality-Virtuality Continuum"', welches von Milgram, Takemura, Utsumi und Kishino welches in Abbildung \ref{rvc_img} zu sehen ist. Dieses stellt bildlich dar, wie sich eine \ac{AR}-Anwendung einordnen lässt. Links ist die reale Umgebung (Real Environment) und rechts die Virtuelle Umgebung (Virtual Environment) zu sehen. 

Heutige \ac{VR}-Anwendungen müssen also ganz rechts eingeordnet werden. Zwischen der realen und der virtuellen Umgebung gibt es jedoch noch weitere Abstufungen. So gibt es die "`Augmented  Reality"', welche die reale Welt um virtuelle Elemente ergänzt. Sie bezieht sich eher auch die reale Umgebung, weshalb sie weitere links angeordnet ist. Die "`Augmented Virtuality"' ist das genaue Gegenteil der \ac{AR}. Hier wird die virtuelle Welt des Computers um reale Gegenstände ergänzt. 

Im wissenschaftlichen Umfeld wird als Oberbegriff für "`Augmented Reality"' und "`Augmented Virtuality"' oft "`Mixed Reality"' oder "`Enhanced Reality"' verwendet. \cite{ARTuP}

\begin{figure}[!ht]
\centering
\includegraphics[width=10cm]{Bilder/ar_vr.png}
\caption{Reality-Virtuality Continuum \cite{rvc}}
\label{rvc_img}
\centering
\end{figure}

Der Begriff der \ac{AV} ist heute weniger gebräuchlich, da es keine wirklichen Anwendungsfelder gibt, in denen eine virtuelle Welt um reale Gegenstände ergänzt werden muss. Im Umkehrschluss wird heute jedoch die \ac{AR} immer bedeutender, da bei fast jeder menschlichen Tätigkeit eine Unterstützung durch Computergenerierte Objekte oder Informationen möglich ist. 

Im medizinischen Umfeld, können Ärzten zum Beispiel Positionen von empfindlichen Gewebe oder zu entfernenden Fremdkörpern bei einer Operation angezeigt werden. Eine andere Einsatzmöglichkeit wäre das unterstützende Anzeigen von Gefahren oder Fluchtwegen für Feuerwehrleute, die sich einem stark verrauchten Brandobjekt befinden. Computer können durch Infrarot und Wärmebild das Sichtvermögen eines Feuerwehrmannes in einer Gefahrensituation so entscheidend erhöhen um sich selbst und andere zu retten.

\subsection{Typen von visuelle Ausgabegeräte}
Im folgenden Abschnitt werden mögliche visuelle Ausgabegerätetypen genauer beschrieben und auf ihre Einsatzmöglichkeiten untersucht. 

\subsubsection{Handheld-Geräte}
Als Handheld werden in Geräte bezeichnet, die wie der Name schon sagt von einer Person in der Hand gehalten werden. Im \ac{AR}-Umfeld sind hier Smartphones oder Tablets gemeint, welche heutzutage durch ihre Dual- oder Quad-Core-Prozessoren und teilweise mehrere Gigabyte RAM in der Lage sind, auch komplexere \ac{AR}-Anwendungen zur Ausführung zu bringen. 

In Smartphones und Tablets nimmt die Kamera das Bild der realen Welt auf und stellt es in Echtzeit auf dem Display dar. Ein Programm (App) ist dann in der Lage, die aufgenommene reale Welt um virtuelle Elemente zu erweitern und diese innerhalb des Kamerabildes auf dem Bildschirm zu platzieren. 

Hierbei ergibt sich jedoch ein Problem, da sich der Blickwinkel es Betrachters vom Blickwinkel der Kamera unterscheidet, was zu einer unsauberen Platzierung der Objekte im Bild führt.
In Abbildung \ref{hh_blick_img} ist zu sehen, wie sich der Blickwinkel der Kamera und der Blickwinkel des Betrachters unterscheiden.

\begin{figure}[!ht]
\centering
\includegraphics[width=10cm]{Bilder/hh_blick.png}
\caption{Unterschiedliche Blickfelder von Betrachter und Kamera bei Handheld-AR \cite{VuAR}}
\label{hh_blick_img}
\centering
\end{figure}

Um den unterschied, zwischen den Blickwinkeln auszugleichen, muss das entsprechende Gerät die beiden Blickwinkel aufeinander abstimmen. Dies kann jedoch kein zur Zeit auf dem Markt befindliches Gerät leisten. 

Einen ersten Ansatz herzu hat die University of California gemacht, indem sie ein Tablet mit einem Kinect Sensor verbunden haben. Mit Hilfe des zusätzlichen Sensors und dem "`KinectFusion Algorithmus"' konnten sie eine "`Magic Lens"' entwickeln, die beide Blickwinkel in etwa aufeinander abstimmt. \cite{MagicLens}

In Abbildung \ref{magic_lens_img} ist einmal der Unterschied zwischen einer Aufnahme mit Magic Lens und ohne schematisch dargestellt. 

Im linken Bildabschnitt ist gut zu sehen, das Bäume und andere Gegenstände im Bildschirm exakt an der Position in der realen Welt liegen. Im Gegensatz dazu sind Bäume und andere Gegenstände im rechten Bildabschnitt merklich verzerrt. 

\begin{figure}[!ht]
\centering
\includegraphics[width=10cm]{Bilder/magic_lens.png}
\caption{Schematische Darstellung des Magic Lens Effekts \cite{VuAR}}
\label{magic_lens_img}
\centering
\end{figure}

Ein weiteres Problem ist, das \ac{AR}-Anwendungen für die optimale Skalierung und Platzierung von virtuellen Objekten im Raum zusätzlich Tiefeninformationen des aktuellen Bildes benötigen, denn nur dann ist auch gewährleistet, dass virtuelle Objekte in der richten Größe im realen Raum positioniert werde. 

Dieses Problem versucht gerade das Projekt Tango von Google zu lösen. Mit Hilfe eines Lasers und einer Infrarotkamera können Tiefeninformationen zum aktuellen Bild aufgenommen und verarbeitet werde. \cite{Tango} 

Nur mit der Tango Technologie ist es bisher möglich, AR-Objekte richtig skaliert und im richtigen Blickwinkel präzise im Raum zu positionieren.  Weitergehende Informationen sind auf den Developer-Seiten\footnote{\url{https://get.google.com/tango/}} von Google zu finden. Bisher unterstützen jedoch nur das "`Lenovo Phab2"' und das "`Asus ZenFon AR"' diese Technologie, da hier eine zusätzliche Sensoreinheit im Smartphone verbaut sein muss.

Abschließen kann über \ac{AR}-Anwendungen ausgesagt werden, das sie Grundsätzlich lauffähig auf Handheld Geräten wie Tablets oder Smartphones sind. Die Bedingungen sind jedoch nicht zumindest heutzutage noch optimal, wie eben dargelegt wurde.

\subsubsection{Video-See-Through-Displays}
Video-See-Through Geräte existieren im Vergleich zu \ac{AR}-Anwendungen schon ziemlich lange. Schon Anfang der 1940er Jahre wurde erste Head-Up-Displays in Kampfflugzeugen eingesetzt, um die Piloten mit zusätzlichen Informationen zu versorgen, welche immer im Blick behalten werden müssen. \cite{WikiHeadup} 

Von Anfang an, wurden die zusätzlichen Informationen auf eine Glasfläche Projiziert, um das Sichtfeld nicht unnötig einzuschränken. Es wurde entweder direkt auf das Cockpit Fenster Projiziert oder die Piloten hatten kleine durchsichtige Displays in den Helm eingearbeitet, auf die mit Hilfe von Spiegeln Projiziert werden konnte.

Mit der Veröffentlichung des iPhones im Jahre 2007 wurde das Thema \ac{AR} immer interessanter, da mit ihm die technische Grundlage gelegt wurde. 2008 erschien der \ac{AR}-Browser Wikitude wodurch das Feld immer mehr Fuß fasste und sich nun rasant weiter entwickelt.
\cite{entwicklerAR}

Die durch die immer größerer Verbreitung von \ac{AR}-Anwendungen wurde auch die Hardware entsprechend weiterentwickelt und angepasst, was zu den ersten \ac{AR}-Brillen führte.

Durch die Kombination von Handheld-Geräten und Head-Up-Displays entstanden bis 2014 die ersten Video-See-Through-Brillen oder auch \ac{AR}-Brillen genannt. Die Entwicklung dieser Brillen steckt noch in den Kinderschuhen, aber dennoch gibt es schon zwei auf dem Markt verfügbare Geräte, die für die spätere Entwicklung eines \ac{NUI} in Betracht gezogen werden können. Zum einen ist das die Epson Moverio BT-200 und zum anderen die Microsoft HoloLens, welche zur Zeit nur für Entwickler verfügbar ist.

Die HoloLens und die Moverio sind beide Prismen basiert. In Abbildung \ref{prisma_img} ist die Funktionsweise von Prismen basierten See-Through-Displays schematisch Dargestellt. Ein normales Display ist an einer Seite eines Prismas angebracht. Das Prisma leitet das Licht des Displays weiter, so das es in das Auge des Betrachters weitergeleitet wird. Weil das Prisma nach vorn durchsichtig ist, wird auch das Umgebungslicht direkt in das Auge des Betrachters geleitet. Innerhalb des Prismas vermischen sich die beiden Bilder, wobei das Displaylicht das Umgebungslicht überlagert. Für den Betrachter sieht es somit aus, als würden die Objekte in der realen Welt zu finden sein.

\begin{figure}[!ht]
\centering
\includegraphics[width=14cm]{Bilder/prisma.png}
\caption{Schematische Darstellung des Aufbaus eines See-Through-Displays \cite{VuAR}}
\label{prisma_img}
\centering
\end{figure}

\paragraph{Die Microsoft HoloLens}
wurde 2015 von Microsoft vorgestellt und ist momentan die Wahrscheinlich beste \ac{AR}-Brille auf dem Markt. Sie verfügt über mehrere Kameras und Tiefensensoren, womit die HoloLens in der Lage ist sich selbstständig im Raum zu orientieren. 

\begin{wrapfigure}{r}{4cm}
\centering
\includegraphics[width=4cm]{Bilder/hololens.jpg}
\caption{Aufgesetzte HoloLens \cite{WikiHolo}}
\label{holo_img}
\vspace{-10pt}
\end{wrapfigure}

Ausgestattet mit einem Intel Atom x5-Z810 Prozessor und 2 Gigabyte RAM, ist die Brille sehr gut aufgestellt und kann Programme und Berechnungen ohne zusätzliche Hardware ausführen. Als Betriebssystem kommt Windows 10 zum Einsatz. Die Akkulaufzeit wird mit zwei Stunden angegeben, wobei dies wie immer Abhängig von der geforderten Leistung ist. \cite{ARVRHolo}

Für Entwickler hat Microsoft ein \ac{SDK} bereitgestellt mit dessen Hilfe es möglich ist Anwendungen für die HoloLens zu entwickeln. \cite{MSHolo}

Über verschiedene Sensoren kann die HoloLens Bewegungen und Gesten des Nutzers wahrnehmen, erkennen und verarbeiten.

Die HoloLens ist standardmäßig in der Lage, grundlegende Gesten zu erkennen, mit denen zum Beispiel ein Tippen, Halten, Rotieren und Zoomen möglich ist.
Auf genauere technische Details zur Programmierung der HoloLens wird im Kapitel ????? genauer eingegangen.

Eine Besonderheit der HoloLens ist natürlich das mit Microsoft ein großer Softwarekonzern hinter der Entwicklung steht und somit ein schnelles voranschreiten der Technik quasi sicher ist.

\paragraph{Die Epson Moverio BT-200}

\subsection{Auswertung der Möglichkeiten für ein Natural User Interface}