\section{Versuch der Entwicklung eines Natural User Interface anhand der Microsoft HoloLens}
Im Kapitel \ref{Augmented Reality} wurde Untersucht, welches Gerät sich am besten zur Entwicklung eines \ac{NUI} eignet um \ac{AR} Inhalte darzustellen und zu bearbeiten. Hierbei ergaben die Untersuchungen, dass die HoloLens das momentan beste Gerät auf dem Markt ist. 

\begin{figure}[!ht]
\centering
\includegraphics[width=14cm]{Bilder/hololens2.jpg}
\caption{Darstellung von \ac{AR}-Inhalten der HoloLens \cite{HoloLensBild}}
\label{HoloLens_example}
\centering
\end{figure}

Um Sprachkommandos innerhalb dieses \ac{NUI} umzusetzen, wurden verschiedene Sprachassistenten miteinander vergliche (siehe Kapitel \ref{Artificial Intelligence}). Am besten eignet sich hierfür Amazon Alexa oder der Google Assistant. 

Das im Kapitel \ref{Beschreibung eines Natural User Interface} beschriebene \ac{NUI} wird nun auf der Microsoft HoloLens umgesetzt. Hierbei wird auf Einschränkungen und Möglichkeiten eingegangen. 

\subsection{Entwicklungsgrundlagen}
Auf der HoloLens läuft ein angepasstes Windows 10, durch welches es möglich wird, jedes Programm auszuführen, welches auf der \ac{UWP} basiert. Im Windows Store gibt es zur Zeit schon einige Anwendungen speziell für die HoloLens.

Im Abschnitt \ref{Erstellung eines Architektur Editor als Prototyp} wurde ein Prototyp spezifiert, welcher im Ansatz dem "`Minecraft Editor"' entspricht. Dieser soll mit Hilfe der Gesten- und Sprach-Erkennung der Microsoft HoloLens um ein \ac{NUI} erweitert werden.

Für die Entwicklung von Apps für die HoloLens wurde die Spieleengine Unity speziell erweitert und unterstützt in der Version 2017.1 die Entwicklung von Anwendungen speziell für die HoloLens.

Um Anwendungen zu Debuggen und auf die HoloLens zu übertragen, wird ein C\# Projekt von Unity erstellt, welche auf die \ac{AR}-Brille übertragen wird. \cite{HoloLensBild}

\subsubsection{Grundlagen}
Der Prototyp (im folgenden WorldBuilder) genannt, hat folgenden Grundaufbau.

Eine Unity-Szene bildet den physischen Raum virtuel in der nach und diesen als Mesh an. Innerhalb deses raumes ist es dann möglich gelbe Blöcke zu setzen. 

Das \ac{NUI} soll nun die Steuerung dieser Szene wie in Abschnitt \ref{Beschreibung der Gesten und Sprachkommandos} übernehmen.

In Abbildung \ref{WorldBuilder} ist die Unity Szene in Aktion zu sehen. Im Hintergrund befindet sich der physische Raum, welcher in die Szene integriert wird. Abgebildet wird dieser Raum innerhalb des WorldBuilder als Mesh um zu zeigen, wie der Raum von der HoloLens erkannt wurde. Dank des erkannten Raumes, können die Blöcke direkt innerhalb des Raumes auf dem Boden oder anderen Oberflächen positioniert werden.

\begin{figure}[!ht]
\centering
\includegraphics[width=14cm]{Bilder/WorldBuilder.jpeg}
\caption{WorldBuilder mit Raum-Mesh und Blöcken}
\label{WorldBuilder}
\centering
\end{figure}

\subsubsection{Gestenentwicklung für die HoloLens}\label{Gestenentwicklung für die HoloLens}
Die HoloLens unterstützt ab Werk zwei Gesten. Zum einen die Select-Geste und zum anderen die Home-Geste. In Abbildung \ref{HoloLens_Select} wird Select in der "`Ready"' Stellung gezeigt. Somit ist der Zeigefinger nach oben gerichtet und die anderen Finger bilden zusammen mit dem Daumen einen Tunnel. 

Wird der Zeigefinger nun auf den Daumen bewegt ist es als würde der Nutzer einen "`Klick"' mit der Maustaste tätigen. Die Position der Hand ist dabei egal. Selektiert wird immer die Schaltfläche auf der der Cursor ruht, welcher sich immer in der Mitte des Sichtfeldes befindet und der sich mit den Kopfbewegungen des Nutzers mitbewegt.

\begin{figure}[!ht]
\centering
\includegraphics[width=6cm]{Bilder/select.png}
\caption{Die Select-Geste der HoloLens \cite{Gestures}}
\label{HoloLens_Select}
\centering
\end{figure}

Die zweite Geste ist die Home-Geste, welche von der HoloLens selbst als "`Bloom"' bezeichnet wird. In Abbildung \ref{HoloLens_Bloom} ist diese Geste dargestellt. Die Handfläche zeigt nach oben wobei die Finger sich alle über der Handfläche berühren. Wird die Hand dann geöffnet, so erkennt die HoloLens die Bloom-Geste und öffnet das Home-Menü der HoloLens, beziehungsweise schließt es, wenn es bereits geöffnet ist.

\begin{figure}[!ht]
\centering
\includegraphics[width=6cm]{Bilder/bloom.png}
\caption{Die Bloom-Geste der HoloLens \cite{Gestures}}
\label{HoloLens_Bloom}
\centering
\end{figure}

Die Select-Geste hat zusätzlich noch zwei Erweiterungen. Wenn der Finger der Select-Geste auf den Daumen bewegt und dort gehalten wird, interpretiert die Brille dies als "`Hold"'. Hold ist gleichbedeutend mit einem langen Klick, über das zum Beispiel ein Kontextmenü angezeigt werden kann. 

Wird die Hand während der Hold-Geste nach oben, unten, links oder rechts bewegt, kann dies als Scrollen oder Zoomen interpretiert werden, wie es in Abbildung \ref{HoloLens_Zoom} zu sehen ist.

\begin{figure}[!ht]
\centering
\includegraphics[width=6cm]{Bilder/zoom.png}
\caption{Die Zoom-Geste oder Scroll-Geste der HoloLens \cite{Gestures}}
\label{HoloLens_Zoom}
\centering
\end{figure}

Die HoloLens unterscheidet also drei Gesten, Select, Bloom und Hold. Um die Lernkurve gering zu halten und Nutzer nicht zu überfordern wurden laut Microsoft die Gesten auf diese drei beschränkt. Das hinzufügen eigener Gesten ist also nicht möglich. \cite{Gesture_Design}

\paragraph{Neudefinition der Gesten} für die HoloLens.
Die im Kapitel \ref{Beschreibung eines Natural User Interface} beschriebenen Gesten können technisch nicht umgesetzt werden. Wodurch die Gesten neu definiert werden müssen um den technischen Anforderungen und den eines \ac{NUI} gerecht zu werden.

\subparagraph{Das Setzen eines Blocks} sollte um den natürlich Fluss der HoloLens gerecht zu werden über die Select-Geste umgesetzt werden. Hierbei sollte der Cursor immer einen Block anzeigen, damit der Nutzer sieht wie er abgesetzt aussieht.

\subparagraph{Das Löschen eines Blocks} wiederum kann entsprechend über eine Hold-Geste umgesetzt werden. Welche beim auslösen den entsprechenden Block löscht.

\subparagraph{Drehen von Objekten} das drehen von einzelnen Blöcken ist nicht Sinnvoll. Das Drehen von verbundenen Blöcken jedoch kann über die Zoom-Geste nach links oder rechts erreicht werden. Hierbei muss natürlich auf die Physik geachtet werden, den Objekte können nicht durch reale Gegenstände bewegt werden.

\subsubsection{Sprachkommandos für die HoloLens}\label{Sprachkommandos für die HoloLens}
Die HoloLens verfügt wie mittlerweile jedes Windows 10 getriebene Gerät über eine Cortana-Instanz, welche den Nutzer unterstützt. Der Funktionsumfang ist jedoch gleich mit einem standard Windows 10. Innerhalb eines Programms kann Cortana jedoch nicht verwendet werden. 

Technisch ist die Verwendung von Amazon Alexa oder dem Google Assistant innerhalb der Unity-Skripte zwar möglich, jedoch nicht zwingend notwendig, da Unity über die \ac{UWP}-API verfügt, verfügt es auch über die darin enthaltene SpeechRecognition-API. \cite{VoiceUnity}

Über die Speech-API können direkt, innerhalb der Unity-Skripte, Schlüsselwörter angegeben werden. Werden sie erkannt, können ihnen zugewiesene Routinen ausgeführt werden.

Dies Funktioniert zusätzlich über eine \ac{SRGS}-Grammatik welche schon im Abschnitt \ref{Beschreibung Sprachkommandos} beschrieben wurde.

Es entfällt bei der Verwendung der HoloLens also ein Sprachassistent wie Alexa oder der Google Assistant und die damit verbundene Entwicklung von Skills. 

\subsection{MixedRealityToolkit}
Das MixedRealityToolkit wird von Microsoft entwickelt und ist eine Sammlung von Skripten und Komponenten, welche Programmierer bei der Entwicklung von Programmen für die HoloLens und Windows Mixed Reality headsets unterstützen. 

Viele Grundlegende Elemente, wie zum Beispiel Inputs, UI Elemente oder andere Helper sind hier schon fertig implementiert. Was wiederum Zeit beim entwickeln spart. \cite{HoloToolKit}

Der Quellcode ist ebenso wie viele Beispiele auf Github\footnote{\url{https://github.com/Microsoft/MixedRealityToolkit-Unity}} zu finden.

\subsection{Spatial Mapping}
Mit Hilfe des "`Spatial Mapping"' ist es möglich, den physischen Raum innerhalb der Unity Szene einzubauen. Hierbei gibt es zwei Möglichkeiten der Implementierung. Zum einen gibt es die Möglichkeit die fertige Skript-Implementierung des  MixedRealityToolkit zu verwenden. Zum anderen können auch die standard Unity-Komponenten "`Spatial Mapping Renderer"' und "`Spartial Mapping Collider"' (siehe Abbildung \ref{Unity_Spatial}) zu verwenden. 

\begin{figure}[!ht]
\centering
\includegraphics[width=10cm]{Bilder/mapping2.png}
\caption{Spatial Mapping in Unity}
\label{Unity_Spatial}
\centering
\end{figure}

Im WordBuilder wurde das Spatial Mapping über Unity-Komponenten umgesetzt, da hier die Implementierung einfacher und übersichtlicher innerhalb der Szene umgesetzt werden kann wie in Abbildung \ref{Unity_Spatial} zu sehen ist. 

Über den "`Spatial Mapping Renderer"' ist es möglich, den physischen Raum, welcher von der HoloLens erkannt wird innerhalb der Szene zu rendern. Hierbei kann natürlich ein beliebiges "`Material"' welches vorher definiert wurde eingesetzt werden.

Der "`Spatial Mapping Collider"' bringt den erkannten Raum als Collider in die Szene. Über ihn ist somit möglich virtuelle Objekte physikalisch korrekt mit dem Raum interagieren zu lassen. Blöcke, welche in die Luft gesetzt werden, fallen somit auf den Boden des Raumes und bleiben liegen.

Das Rendern des physischen Raumes innerhalb der Szene ist nicht vorgesehen. Jedoch wurde es zu Entwicklungszwecken implementiert. Über Sprachbefehle wie "`Show Room"' oder "`Hide Mesh"' kann der erkannte Raum ein oder ausgeblendet werden. Die Befehle sind konform des \ac{NUI} implementiert und können auf unterschiedliche weise ausgesprochen werden. In Abbildung \ref{WorldBuilder} ist das eingeblendete Mesh zu sehen. 

% \lstinputlisting[language=C,caption=Angular View im HTML-DOM \cite{W3CAngular}, label=Angular View, captionpos=b]{Code/CubeBehaviour.cs}

\subsection{Das NUI Skript}
Die Start Methode, ist in Unity-Skripten der Konstruktor, welche einmal zum Start der Szene aufgerufen wird.

Im ersten Teil des NUI-Skripts \ref{NUI_Start} (Zeilen 3-13) werden alle vom Skript benötigten Komponenten geladen. In Zeile 13 werden über einen Methodenaufruf alle "`Keywords"' generiert, auf welche das NUI reagieren soll. Daraufhin, wird der "`KeywordRecognizer"' mit den erstellten Keywords erstellt und gestartet (Zeilen 17-21).

Als nächster Schritt, wird der "`GestureRecognizer"' erstellt und definiert, was bei einer erkannten Tap-Geste geschehen soll (Zeile 24-25). Wird ein Tap erkannt, wird ein Raycast von der Kameramitte ausgeführt. Trifft der Raycast ein Objekt so wird ein neuer Würfel gesetzt oder der getroffene Würfel entfernt, je nachdem ob der Nutzer gerade im Setzen- oder Entfernenmodus ist.

\lstinputlisting[language=C,caption=Start Methode des NUI-Skripts, label=NUI_Start, captionpos=b]{Code/NUI_Start.cs}

Im weiteren Verlauf des Skrips sind unzählige Keywords definiert und was passieren soll, wenn der KeywordRecognizer diesen Befehl erkennt. Im Listing \ref{NUI_Keyword} ist dies einmal beispielhaft dargestellt. Es wird dem "`Dictionary"' "`keywords"' ein Schlüssel-Wert-Paar hinzugefügt. Dies ist zum einen der Befehl als String und die Definition, welche ausgeführt wird wenn das Keyword erkannt wird. Im Beispiel wird die Methode "`SelectYellowCube"' ausgeführt.

\lstinputlisting[language=C,caption=Keyword Erstellung, label=NUI_Keyword, captionpos=b]{Code/NUI_Keyword.cs}

Wählt der Nutzer einen Würfeltyp aus, wird das entsprechende Prefab geladen und dem Cursor übergeben. Zusätzlich wird der UIController auf den Würfeltyp umgestellt und der SpeechController, welcher Sprachausgaben macht, informiert (Listing \ref{NUI_Select}).

\lstinputlisting[language=C,caption=Auswählen eines Würfels, label=NUI_Select, captionpos=b]{Code/NUI_Select.cs}

\subsection{Der SpeechController}
Die Hauptaufgabe des "`SpeechController"' ist das heranführen des Nutzers an das Programm. Wird der WorldBuilder gestartet begrüßt der SpeechController den Nutzer und gibt eine kurze Einführung in die Bedienung. 

Die Text zu Sprache Funktion ist Bestandteil des MixedRealityToolkit, wobei vier Stimmen zur Auswahl stehen. Im Listing \ref{Speech} ist die erste Initialisierung zu sehen. Es wird in Zeile 4 eine Stimme ausgewählt. Danach kann ein Text übergeben werden, welcher durch Sprachsynthese eine Audioausgabe generiert. 

\lstinputlisting[language=C,caption=Testausgabe, label=Speech, captionpos=b]{Code/Speech.cs}

"`TextToSpeech"' kann zur Zeit nur in englisch genutzt werden. Zwar kann auch ein anderssprachiger Satz eingegeben werden, hierbei ist aber die Ausgabe schlecht, da das Framework auf englischer Sprachbetonung basiert. 

Damit sich die Sprachausgabe nicht überlagert muss vor der Verarbeitung des zweiten Textes geprüft werden, ob die vorherige Ausgabe beendet ist. Dies geschieht über zwei Methoden, wie im Listing \ref{Speech_While} beispielhaft zu sehen ist. 

Die obere Methode ruft die untere als Co-Routine auf. Hierbei wird der Aufruf solange wiederholt bis die Bedingung in Zeile 8 erfüllt ist. Also bis kein Text mehr in der Schlange ist und die Sprachausgabe beendet wurde. Ist das dann der Fall, wird einmalig der Eingabetext in Zeile 11 Synthetisiert. 

Innerhalb des SpeechController-Skripts bedinden sich viele solcher Methodenpaare, welche von anderen Skripten aufgerufen werden können.

\lstinputlisting[language=C,caption=Testausgabe mit Überlagerungssperre, label=Speech_While, captionpos=b]{Code/Speech_While.cs}

\subsection{Der WorldCursor}
Der WorldCursor zeigt die aktuelle Position des Cursors an. Hierfür macht der Cursor in der Update-Methode, welche einmal pro Frame durchlaufen wird, einen Raycast. 

Der Raycast hat drei mögliche Funktionen, welche er abdecken muss. Trifft er kein Objekt, so ist der Raum nicht vollständig geladen und es kann kein Würfel platziert werden. In diesem Fall, wird die Nachricht "`Loading Room! Please Wait ..."', mittig im Sichtfeld des Nutzers, angezeigt.

Trifft der Raycast einen Punkt, so wird ein Würfel an der entsprechenden Position angezeit. Hierbei sieht der Würfel genau aus, wie der, welcher bei einer Tap-Geste platziert wird. Ist jedoch der Remove-Modus aktiviert, in dem Würfel entfernt werden können, wird an Stelle des Würfels ein kreisförmiger Cursor angezeigt, welcher bei einer Tap-Geste den anvisierten Würfel löscht.

Im Listing \ref{World_Update} wird in Zeile 8 der Raycast durchgeführt. Trifft dieser, wir abhängig vom aktuellen Zustand, entweder der Remove-Cursor oder der Würfel-Cursor angezeigt. In den Zeilen 12-19 wird der Remove-Cursor eingeblendet und entsprechend gedreht, so das dieser immer auf der Oberfläche des anvisierten Objektes ist. 

Ist der Remove-Modus deaktiviert, so wird in den Zeilen 23-29 der Würfel-Cursor aktiviert und zum richtigen Punkt bewegt.

Trifft der Raycast nicht, so wird in den Zeilen 34-37 der Text der Nachricht, welche angibt das der Raum geladen wird, angezeigt.

\lstinputlisting[language=C,caption=Anzeigen des WorldCursor, label=World_Update, captionpos=b]{Code/World_Update.cs}

\subsection{Der UIController}
Der UIController ist eine Unterstützung der Anzeige, in welchem Modus der Nutzer sich zur Zeit befindet.

Es wird im oberen linken Bildschirmbereich ein Quadrat in der aktuellen Farbe der Würfel angezeigt. Außerdem ist das Quadrat im Remove-Modus durchgestrichen. 

Die Anzeige dient lediglich zur Unterstützung des Nutzers, hat aber keine funktionale Bedeutung. 
Der Code besteht aus Standard-Unity-Prefabs und der Quellcode des Skriptes ist trivial, weshalb hier auf ein Programmierbeispiel verzichtet wird.

\subsection{Probleme bei der Entwicklung}
In den folgenden Abschnitten werden festgestellte Probleme bei der Entwicklung und mögliche Lösungen beschrieben.

\subsubsection{Ungenauigkeiten bei der der Meshaufzeichnung}
Die HoloLens scannt wärend des gesammten Betirebs unablässig die Umgebung und passt das Mesh immer wieder an. Hierdurch verändert sich der Untergund der Blöcke, welcher durch das Mesh abgebildet wird, was dazu führt das die Blöcke innerhalb der Szene zittern und sich gegebenfalls bewegen, wenn das Mesh aktualisiert wird.

Ein Lösung des Problems wäre zum Beispiel die Physik der Blöcke abzuschalten, damit diese nicht auf die Bewegung des Bodens reagieren. 

\subsubsection{Gestenlimitierung der HoloLens}
Im Abschnitt \ref{Gestenentwicklung für die HoloLens} wurde beschrieben, dass die HoloLens nur die drei Gesten Select, Bloom und Hold beherrscht. Diese Limitierung ist nicht technischer Natur, sondern die Beschränkung wurde eingeführt um Nutzer nicht zu überfordern. 

Bei der Entwicklung eines \ac{NUI} ist diese Beschränkung jedoch eher hinderlich. Zum Vergleich können Geräte wie zum Beispiel die "`Leap Motion"' viel mehr Gesten erkennen und verarbeiten. Die Leap Motion verfügt über eine Unity Integration, durch welche es Grundsätzlich möglich wäre Gesten zu erkennen und diese innerhalb der HoloLens Szene zu verarbeiten. \cite{LeapMotion}

\subsubsection{Sprachkommando Limitierung}
Die HoloLens verfügt zum einen über Cortana, einen Assistenten, welcher aktuell nicht erweitern lässt. Aus diesem Grund, kann er auch nicht in Programme integriert werden. 

Wie in Abschnitt \ref{Sprachkommandos für die HoloLens} schon beschrieben ist es möglich über die in Unity integrierte \ac{UWP} Sprachkommandos zu implementieren. Hierbei gibt es aber auch eine Limitierung. Aktuell unterstützt Unity nur englischsprachige Kommandos. 

Da es sich bei der HoloLens aktuell um eine Entwickler-Gerät handelt, welches nicht auf dem freien Markt gibt ist diese Limitierung aber nicht dramatisch. Dass Interface muss lediglich mit englischer Sprache bedient werden. Die Umsetzung anderer Sprache ist aber abzusehen, da auch der Assistent Cortana schon in der deutschen Sprache verfügbar ist.

\subsection{Auswertung des Programm in Hinsicht auf das geplante NUI}
Im Kapitel \ref{Natural User Interfaces} wurde ein \ac{NUI} grundlegend beschrieben. Hierbei wurde erklärt, welche Dinge zu beachtet sind und wie eine Umsetzung geplant werden kann.

Der Abschnitt \ref{Erstellung eines Architektur Editor als Prototyp} wurde hierauf aufbauend ein konkretes \ac{NUI} am Beispiel eines prototypischen Architektur Editors beschrieben. Es wurde genaustens Definiert, wie Sprachbefehle und Gesten umgesetzt werden müssen um ein möglichst "`Natürliches"' Interface zu erstellen, welches einfach zu erlernen und zu bedienen ist.

Im Abschnitt \ref{Gestenentwicklung für die HoloLens} wurde jedoch gezeigt, dass es nicht möglich ist zusätzliche Gesten für die HoloLens zu entwickeln. Woraufhin die Definition der Gesten angepasst werden musste.

Auch wenn die Entwicklung der Gesten nicht wie im geplanten Maße umgesetzt werden konnte, so konnten jedoch die erdachten Sprachkommandos umgesetzt werden. Da die HoloLens momentan noch nicht auf den freien Markt erhältlich ist, ist die aktuelle Spracherkennung lediglich auf die englische Sprache beschränkt. Um zu zeigen, dass die Entwicklung eines \ac{NUI} möglich ist, ist diese Einschränkung jedoch nicht von Bedeutung. 

Mit der reinen HoloLens ist es also nicht möglich, eine AR-Anwendung zu entwickeln die mit frei erdachten Gesten arbeitet. 